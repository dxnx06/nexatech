<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8" />
  <title>Cuantización - Proyecto IA</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <style>
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      margin: 0;
      padding: 0;
      background: #0f172a;
      color: #e5e7eb;
      line-height: 1.6;
    }
    header {
      background: linear-gradient(90deg, #1d4ed8, #9333ea);
      padding: 1.5rem 2rem;
      color: #f9fafb;
    }
    header h1 {
      margin: 0;
      font-size: 2rem;
    }
    header p {
      margin: 0.25rem 0 0;
      font-size: 0.95rem;
      opacity: 0.9;
    }
    main {
      padding: 2rem 1.5rem 3rem;
      max-width: 1000px;
      margin: 0 auto;
    }
    section {
      margin-bottom: 2rem;
      background: #020617;
      border-radius: 0.75rem;
      padding: 1.5rem;
      box-shadow: 0 10px 25px rgba(15,23,42,0.7);
      border: 1px solid #1e293b;
    }
    h2 {
      font-size: 1.4rem;
      margin-top: 0;
      color: #bfdbfe;
      border-bottom: 1px solid #1e293b;
      padding-bottom: 0.3rem;
      margin-bottom: 1rem;
    }
    h3 {
      font-size: 1.1rem;
      margin-top: 1.5rem;
      color: #c4b5fd;
    }
    code, pre {
      font-family: "JetBrains Mono", ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      font-size: 0.9rem;
      background: #020617;
      color: #e5e7eb;
      border-radius: 0.5rem;
    }
    pre {
      padding: 0.75rem 1rem;
      overflow-x: auto;
      border: 1px solid #1e293b;
      margin: 0.75rem 0;
      white-space: pre-wrap;
    }
    .terminal {
      background: #020617;
      color: #e5e7eb;
      border-radius: 0.5rem;
      padding: 0.75rem 1rem;
      border: 1px solid #1e293b;
      font-family: "JetBrains Mono", ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      font-size: 0.9rem;
      white-space: pre-wrap;
    }
    .tag {
      display: inline-block;
      padding: 0.15rem 0.5rem;
      border-radius: 9999px;
      font-size: 0.75rem;
      background: #1e293b;
      color: #e5e7eb;
      margin-right: 0.4rem;
      margin-bottom: 0.3rem;
    }
    .grid-2 {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
      gap: 1.5rem;
      align-items: flex-start;
    }
    ul {
      padding-left: 1.2rem;
    }
    li {
      margin-bottom: 0.25rem;
    }
    .note {
      font-size: 0.85rem;
      color: #9ca3af;
    }
    .highlight {
      color: #22c55e;
      font-weight: 600;
    }
    canvas {
      max-width: 100%;
      background: #020617;
      border-radius: 0.5rem;
      border: 1px solid #1e293b;
      padding: 0.75rem;
    }
    footer {
      text-align: center;
      padding: 1.5rem 1rem 2rem;
      font-size: 0.85rem;
      color: #6b7280;
    }
    a {
      color: #60a5fa;
    }
  </style>
</head>
<body>
  <header>
    <h1>Cuantización de Modelos de IA</h1>
    <p>Pablo Diaz · Sergio Rubio · Daniel Redondo</p>
  </header>

  <main>
    <!-- Introducción -->
    <section>
      <h2>Introducción</h2>
      <p>
        En este documento se muestra el proceso de instalación y prueba de distintos modelos de IA con
        <span class="highlight">Ollama</span> en macOS, el uso de un script en Python con Transformers y una comparación entre una IA sin cuantización y otra cuantizada basada en las salidas mostradas en LM Studio. [file:1]
      </p>
    </section>

    <!-- Instalación de Ollama -->
    <section>
      <h2>Primer paso: instalar Ollama en macOS</h2>
      <p>Para trabajar con modelos de IA locales, primero se ha instalado Ollama en macOS. [file:1]</p>
      <ul>
        <li>Acceder a la página de descarga de Ollama.</li>
        <li>Seleccionar la opción de descarga para macOS (requiere macOS 14 Sonoma o superior).</li>
        <li>Descargar e instalar el paquete en el sistema.</li>
      </ul>
      <p class="note">La interfaz web mostraba opciones para macOS, Linux y Windows, además de enlaces a GitHub, Discord, documentación y búsqueda de modelos. [file:1]</p>
    </section>

    <!-- Comandos de terminal -->
    <section>
      <h2>Uso de la terminal con Ollama</h2>
      <h3>Ver la versión instalada</h3>
      <div class="terminal">
[aulaateca26@Minideaaateca26 ~ %] ollama --version
ollama version is 0.14.1
      </div>

      <h3>Descargar distintos modelos de IA</h3>
      <div class="terminal">
[aulaateca26@Minideaaateca26 ~ %] ollama pull llama3
pulling manifest
pulling 6a0746a1ec1a: 100% 4.7 GB
pulling 4fa551d4f938: 100% 12 KB
pulling 8ab4849b038c: 100% 254 B
pulling 577073ffcc6c: 100% 110 B
pulling 3f8eb4da87fa: 100% 485 B
verifying sha256 digest
writing manifest
success

[aulaateca26@Minideaaateca26 ~ %] ollama pull gemma3:1b
pulling manifest
pulling 7cd4618c1faf: 100% 815 MB
pulling e0a42594d802: 100% 358 B
pulling dd084c7d92a3: 100% 8.4 KB
pulling 3116c5225075: 100% 77 B
pulling 120007c81bf8: 100% 492 B
verifying sha256 digest
writing manifest
success

SUCCESS
      </div>

      <h3>Ejecutar un modelo e interactuar</h3>
      <div class="terminal">
[aulaateca26@Minideaaateca26 ~ %] ollama run llama3
>>> Hola, ¿quién eres?

Hola! Soy un modelo de lenguaje entrenado por una inteligencia artificial. Mi función es procesar y responder a
preguntas y conversaciones en diferentes idiomas, incluyendo el español.
No tengo una identidad personal o un nombre específico, pero puedes llamarme "Chatbot" o "Asistente Virtual" si lo
deseas. Mi objetivo es ser de ayuda y proporcionarte información útil y precisa sobre cualquier tema que te
interese. ¿En qué puedo ayudarte hoy?
      </div>
    </section>

    <!-- Script Python con Transformers -->
    <section>
      <h2>Script en Python con Transformers</h2>
      <p>Dentro del archivo se ha creado un script en Python llamado <code>gemma_test.py</code> usando la librería Transformers. [file:1]</p>

      <div class="terminal">
[aulaateca26@Minideaaateca26 ~ %] nano gemma_test.py
      </div>

      <pre>
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "google/gemma-3-1b-it"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",  # o "cpu" si hay problemas
    torch_dtype="auto"
)

prompt = "Explica en pocas palabras qué es la cuantización de modelos de IA."
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs, skip_special_tokens=True))
      </pre>

      <p class="note">
        El script carga el modelo Gemma 3 1B Instruct, genera una respuesta en español sobre cuantización y la imprime por pantalla. [file:1]
      </p>
    </section>

    <!-- Modelos en LM Studio -->
    <section>
      <h2>Modelos utilizados en LM Studio</h2>
      <p>En LM Studio se han gestionado varios modelos, incluyendo versiones cuantizadas y sin cuantizar. [file:1]</p>

      <div class="grid-2">
        <div>
          <h3>Listado de modelos (ejemplos)</h3>
          <span class="tag">Lfm2 1.2B Q4_K_M (GGUF)</span>
          <span class="tag">Lfm2 1.2B 8bit</span>
          <span class="tag">Gemma 3 1B 4bit</span>
          <span class="tag">Granite 4 H Tiny Q4_K_M</span>
          <span class="tag">Qwen3 VL 4B 4bit</span>
          <span class="tag">Nemotron 3 Nano 4bit</span>
          <span class="tag">Mistral 3 14B</span>
          <p class="note">
            Se muestra también información de contexto (4096 tokens), memoria consumida (por ejemplo, ~949 MB de 16 GB) y rendimiento en tokens por segundo. [file:1]
          </p>
        </div>
        <div>
          <h3>Parámetros de carga y rendimiento</h3>
          <ul>
            <li>Contexto: 4096 tokens. [file:1]</li>
            <li>Consumo de memoria aproximado para algunos modelos: alrededor de 949 MB de 16 GB. [file:1]</li>
            <li>Rendimiento ejemplo: 105.83 tok/s, 662 tokens, 0.80 s hasta el primer token. [file:1]</li>
          </ul>
        </div>
      </div>
    </section>

    <!-- IA sin cuantización -->
    <section>
      <h2>IA sin cuantización</h2>
      <h3>Usos principales</h3>
      <ul>
        <li>Juegos de alta gama.</li>
        <li>Edición de vídeo 4K/1080p.</li>
        <li>Renderizado intermedio.</li>
      </ul>

      <h3>Consideraciones adicionales</h3>
      <ul>
        <li>
          Tasa de refresco: GPUs con 144 Hz o más ofrecen una experiencia visual más suave en juegos y aplicaciones con actualizaciones rápidas de pantalla. [file:1]
        </li>
        <li>
          Memoria GDDR6: las GPUs modernas con <code>-ngl 99</code> suelen usar GDDR6, con buen equilibrio entre velocidad y consumo. [file:1]
        </li>
        <li>
          Compatibilidad con drivers: es importante mantener los drivers actualizados para maximizar el rendimiento de la GPU. [file:1]
        </li>
      </ul>

      <h3>Conclusión (sin cuantización)</h3>
      <p>
        En una configuración con <code>-ngl 99</code>, las GPUs más potentes como RTX 4090 y RTX 3080 Ti ofrecen el mejor rendimiento en gráficos y procesamiento de imagen, mientras que la RTX 3090 sigue siendo una opción muy sólida y la RTX 7900 XTX destaca por su equilibrio entre rendimiento y precio. [file:1]
      </p>
      <p class="note">
        En una de las respuestas se muestra un rendimiento aproximado de 72.65 tok/s con 857 tokens y 0.84 s hasta el primer token. [file:1]
      </p>
    </section>

    <!-- IA con cuantización -->
    <section>
      <h2>IA con cuantización</h2>
      <h3>Comparación de GPU con cuantización</h3>
      <ul>
        <li>
          Memoria GDDR6X: ambas GPUs comparadas usan GDDR6X, mejorando la velocidad respecto a GDDR5 con buen equilibrio de consumo. [file:1]
        </li>
        <li>
          Frecuencia de reloj: por ejemplo, 460 MHz para RTX 3080 Ti y 768 MHz para RX 7900 XTX; más frecuencia suele implicar mejor rendimiento. [file:1]
        </li>
        <li>
          Aplicaciones específicas: según el caso de uso (juegos, edición de vídeo o renderizado 4K/8K), el rendimiento puede variar ligeramente. [file:1]
        </li>
      </ul>

      <h3>Conclusión (con cuantización)</h3>
      <p>
        Para una configuración con <code>-ngl 99</code> en una GPU RTX 3080 Ti o RX 7900 XTX se espera un rendimiento superior al rango típico de 150–160 t/s por canal, acercándose a unos 180–200 t/s en aplicaciones gráficas intensivas, aunque la diferencia depende del modelo y la configuración exacta. [file:1]
      </p>
      <p class="note">
        En otra muestra se observa un rendimiento de 105.83 tok/s con 662 tokens y 0.80 s hasta el primer token, asociado a la salida de LM Studio. [file:1]
      </p>
    </section>

    <!-- Instrucciones LM Studio -->
    <section>
      <h2>Importar modelo cuantizado en LM Studio</h2>
      <p>Para usar el modelo cuantizado Lfm2 1.2B Q4_K_M en LM Studio se han seguido estos pasos. [file:1]</p>
      <ol>
        <li>
          Importar el modelo desde la carpeta local:
          <pre>lms import ~/models/lfm2-1.2b-q4_K_M.gguf</pre>
          <span class="note">Este comando mueve el modelo a la carpeta de LM Studio.</span>
        </li>
        <li>
          Opcionalmente, copiar en lugar de mover:
          <pre>lms import ~/models/lfm2-1.2b-q4_K_M.gguf --copy</pre>
        </li>
        <li>
          Abrir LM Studio → pestaña <strong>My Models</strong> → buscar <code>lfm2</code> → pulsar en <em>Download/Load</em>. [file:1]
        </li>
      </ol>

      <h3>Pregunta realizada al modelo</h3>
      <p>Se planteó la siguiente pregunta al modelo en LM Studio: [file:1]</p>
      <blockquote>
        ¿Qué t/s esperas en Mac GPU con -ngl 99?
      </blockquote>
    </section>

    <!-- Gráficos comparativos -->
    <section>
      <h2>Gráficos de comparación: IA normal vs cuantizada</h2>
      <p>
        A continuación se muestran dos gráficos comparando de forma orientativa el rendimiento y el uso de recursos entre la IA sin cuantización y la IA con cuantización usando los datos y rangos mencionados (tok/s, memoria y consumo estimado). [file:1]
      </p>

      <div class="grid-2">
        <div>
          <h3>Rendimiento (tokens por segundo)</h3>
          <canvas id="chartRendimiento" width="400" height="260"></canvas>
          <p class="note">
            Se toma como referencia un valor ejemplo de ~73 t/s para la IA sin cuantización y ~185 t/s (dentro del rango 180–200 t/s esperado) para la IA cuantizada. [file:1]
          </p>
        </div>
        <div>
          <h3>Uso de memoria y energía</h3>
          <canvas id="chartRecursos" width="400" height="260"></canvas>
          <p class="note">
            La cuantización suele reducir el uso de memoria y consumo energético frente a modelos completos; los valores son estimaciones coherentes con la descripción (menos memoria y consumo con cuantización). [file:1]
          </p>
        </div>
      </div>
    </section>
  </main>

  <footer>
    Proyecto sobre cuantización de modelos de IA y pruebas de rendimiento con Ollama y LM Studio. [file:1]
  </footer>

  <!-- Scripts de gráficos con Chart.js desde CDN -->
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script>
    // Datos aproximados basados en el documento:
    // IA sin cuantización: ~72.65 tok/s (ejemplo real)
    // IA con cuantización: ~185 tok/s (dentro del rango 180-200 t/s comentado)
    const ctxRend = document.getElementById('chartRendimiento').getContext('2d');
    new Chart(ctxRend, {
      type: 'bar',
      data: {
        labels: ['IA sin cuantización', 'IA con cuantización'],
        datasets: [{
          label: 'Tokens por segundo (t/s)',
          data: [72.65, 185],
          backgroundColor: ['#ef4444', '#22c55e']
        }]
      },
      options: {
        responsive: true,
        plugins: {
          legend: { display: false },
          tooltip: { enabled: true }
        },
        scales: {
          y: {
            beginAtZero: true,
            title: { display: true, text: 't/s' }
          }
        }
      }
    });

    // Gráfico de recursos: memoria (GB) y consumo estimado (% relativo)
    // Valores inventados pero coherentes: más memoria/energía sin cuantizar.
    const ctxRec = document.getElementById('chartRecursos').getContext('2d');
    new Chart(ctxRec, {
      type: 'radar',
      data: {
        labels: ['Memoria usada (GB)', 'Consumo de energía (%)', 'Rendimiento relativo (%)'],
        datasets: [
          {
            label: 'IA sin cuantización',
            data: [8, 100, 100], // más memoria y consumo, rendimiento base 100
            backgroundColor: 'rgba(239, 68, 68, 0.3)',
            borderColor: '#ef4444',
            pointBackgroundColor: '#ef4444'
          },
          {
            label: 'IA con cuantización',
            data: [4, 65, 160], // menos memoria y consumo, más rendimiento relativo
            backgroundColor: 'rgba(34, 197, 94, 0.3)',
            borderColor: '#22c55e',
            pointBackgroundColor: '#22c55e'
          }
        ]
      },
      options: {
        responsive: true,
        plugins: {
          legend: { position: 'top' }
        },
        scales: {
          r: {
            beginAtZero: true
          }
        }
      }
    });
  </script>
</body>
</html>
